#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
LLM å·¥å‚æ¨¡å—
è´Ÿè´£åˆ›å»ºå’Œé…ç½®å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯ (å¦‚ Google Gemini, OpenAI ç­‰)ã€‚
"""

import os
from autogen_ext.models.openai import OpenAIChatCompletionClient
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def get_gemini_client(model_name: str = "gemini-3-pro-preview", temperature: float = 0.7):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä¸€ä¸ªé…ç½®å¥½è¿æ¥ Google Gemini çš„ ModelClientã€‚
    
    :param model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "gemini-3-pro-preview"
    :param temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ (0.0 - 1.0)
    :return: é…ç½®å¥½çš„ OpenAIChatCompletionClient å®ä¾‹
    """
    # è·å– API Key
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ [LLM Factory] è­¦å‘Š: æœªæ‰¾åˆ° GEMINI_API_KEY")

    print(f"ğŸ”Œ [LLM Factory] æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {model_name}...")

    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        # ä½¿ç”¨ OpenAIChatCompletionClient é€‚é… Gemini çš„ OpenAI å…¼å®¹æ¥å£
        client = OpenAIChatCompletionClient(
            model=model_name,
            api_key=api_key,
            # æŒ‡å‘ Google çš„ OpenAI å…¼å®¹æ¥å£
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",

            # ğŸ”¥ å¿…é¡»åŒ…å« model_infoï¼Œå¦åˆ™ AutoGen å¯èƒ½æŠ¥é”™æˆ–æ— æ³•æ­£ç¡®è¯†åˆ«æ¨¡å‹èƒ½åŠ›
            model_info={
                "vision": True,             # æ”¯æŒè§†è§‰èƒ½åŠ›
                "function_calling": True,   # æ”¯æŒå‡½æ•°è°ƒç”¨
                "json_output": True,        # æ”¯æŒ JSON è¾“å‡ºæ¨¡å¼
                "structured_output": True,  # æ”¯æŒç»“æ„åŒ–è¾“å‡º
                "family": "gemini"          # æ¨¡å‹å®¶æ—æ ‡è¯†
            },

            temperature=temperature,
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢ç½‘ç»œæ³¢åŠ¨å¯¼è‡´æ–­è¿
            timeout=120
        )
        return client
    except Exception as e:
        print(f"âŒ [LLM Factory] åˆå§‹åŒ–å¤±è´¥: {e}")
        raise e


if __name__ == "__main__":
    pass
