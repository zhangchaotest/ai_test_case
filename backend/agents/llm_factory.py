#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
@Project ï¼šai_test_case_fast 
@File    ï¼šllm_factory.py
@Author  ï¼šå¼ è¶…
@Date    ï¼š2025/12/17 16:14
@Desc    ï¼š
"""

import os
from autogen_ext.models.openai import OpenAIChatCompletionClient
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def get_gemini_client(model_name: str = "gemini-3-pro-preview", temperature: float = 0.7):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä¸€ä¸ªé…ç½®å¥½è¿æ¥ Google Gemini çš„ ModelClientã€‚
    """
    """
        è¿”å›é…ç½®å¥½çš„ Gemini å®¢æˆ·ç«¯
        """
    # è·å– Key
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ [LLM Factory] è­¦å‘Š: æœªæ‰¾åˆ° GEMINI_API_KEY")

    print(f"ğŸ”Œ [LLM Factory] æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {model_name}...")

    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = OpenAIChatCompletionClient(
            model=model_name,
            api_key=api_key,
            # æŒ‡å‘ Google çš„ OpenAI å…¼å®¹æ¥å£
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",

            # ğŸ”¥ 2. å¿…é¡»åŒ…å« model_info (é˜²æ­¢æŠ¥é”™ model_info is required)
            model_info={
                "vision": True,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,  # ğŸ”¥ åŠ ä¸Šè¿™ä¸ªç”± False æ”¹ä¸º True æˆ–åŠ ä¸Šï¼Œæ¶ˆé™¤ Warning
                "family": "gemini"
            },

            temperature=temperature,
            # é˜²æ­¢ç½‘ç»œæ³¢åŠ¨å¯¼è‡´æ–­è¿
            timeout=120
        )
        return client
    except Exception as e:
        print(f"âŒ [LLM Factory] åˆå§‹åŒ–å¤±è´¥: {e}")
        raise e


if __name__ == "__main__":
    pass
